Classification
==============

Overview
--------  
In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into "spam" or "non-spam" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.).

Terms
-----  
 - [**Feature Vector**](https://www.wikiwand.com/en/Feature_vector) - In pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis.
 - [**Linear combination**](https://www.wikiwand.com/en/Linear_combination) - In mathematics, a linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of x and y would be any expression of the form ax + by, where a and b are constants).


Algorithms
----------  
Examples of classification algorithms.

 - **Linear classifiers**
  - In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to. A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.
  - [Wikipedia](https://www.wikiwand.com/en/Linear_classifier)
  - **Examples:**
      - **Fisher's linear discriminant**  
        - Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.  
        - [Wikipedia](https://www.wikiwand.com/en/Linear_discriminant_analysis)
      - **Logistic regression**
        - In statistics, logistic regression, or logit regression, or logit model is a regression model where the dependent variable (DV) is categorical. This article covers the case of binary dependent variablesâ€”that is, where it can take only two values, such as pass/fail, win/lose, alive/dead or healthy/diseased.
        - [Wikipedia](https://www.wikiwand.com/en/Logistic_regression)
      - **Naive Bayes classifier**
        - In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes has been studied extensively since the 1950s.
        - [Wikipedia](https://www.wikiwand.com/en/Naive_Bayes_classifier)
      - **Perceptron**
        - In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another. It is a type of linear classifier, i.e.
        - [Wikipedia](https://www.wikiwand.com/en/Perceptron)
 - **Support vector machines**
  - In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier.
  - [Wikipedia](https://www.wikiwand.com/en/Support_vector_machine)
  - **Example:**
      - **Least squares support vector machines**
        - Least squares support vector machines (LS-SVM) are least squares versions of support vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis. In this version one finds the solution by solving a set of linear equations instead of a convex quadratic programming (QP) problem for classical SVMs.
        - [Wikipedia](https://www.wikiwand.com/en/Least_squares_support_vector_machine)
 - **Quadratic classifiers**
  - A quadratic classifier is used in machine learning and statistical classification to separate measurements of two or more classes of objects or events by a quadric surface. It is a more general version of the linear classifier.
  - [Wikipedia](https://www.wikiwand.com/en/Quadratic_classifier)
 - **Kernel estimation**
  - In statistics, adaptive or "variable-bandwidth" kernel density estimation is a form of kernel density estimation in which the size of the kernels used in the estimate are varied depending upon either the location of the samples or the location of the test point.
  - [Wikipedia](https://www.wikiwand.com/en/Variable_kernel_density_estimation#Use_for_statistical_classification)
  - **Example:**
      - **k-nearest neighbor**
        - In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space.
        - [Wikipedia](https://www.wikiwand.com/en/K-nearest_neighbor_algorithm)
 - **Boosting (meta-algorithm)**
  - Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): Can a set of weak learners create a single strong learner?
  - [Wikipedia](https://www.wikiwand.com/en/Boosting_(meta-algorithm))
 - **Decision trees**
  - **Example:**
      - **Random forests**
        - Random forests is a notion of the general technique of random decision forests that are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.
        - [Wikipedia](https://www.wikiwand.com/en/Random_forest)
 - **Neural networks**
  - In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) and are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other.
  - [Wikipedia](https://www.wikiwand.com/en/Artificial_neural_networks)
 - **Learning vector quantization**
  - In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.
  - [Wikipedia](https://www.wikiwand.com/en/Learning_vector_quantization)


Examples
--------



Resources
---------


Important People 
----------------


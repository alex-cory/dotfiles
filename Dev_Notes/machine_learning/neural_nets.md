Artificial Neural Networks (ANN)
================================
### (aka Neural Nets)

In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) and are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other. The connections have numeric weights that can be tuned based on experience, making neural nets adaptive to inputs and capable of learning.  Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and the biological architecture of the brain is debated; it's not clear to what degree artificial neural networks mirror brain function ([src](http://uhaweb.hartford.edu/compsci/neural-networks-definition.html)).

An ANN is typically defined by three types of parameters:
 1. The interconnection pattern between the different layers of neurons
 2. The learning process for updating the weights of the interconnections
 3. The activation function that converts a neuron's weighted input to its output activation.

The cost function `C` is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.

```
C = cost
E = experience
T = tasks
P = performance
```

### Parts  
 - [**Autoencoder**](https://www.wikiwand.com/en/Autoencoder)
  - An autoencoder, autoassociator or Diabolo network is an artificial neural network used for learning efficient codings. The aim of an auto-encoder is to learn a compressed, distributed representation (encoding) for a set of data, typically for the purpose of dimensionality reduction.
 - [**Deep learning**](./deep_learning.md)
  - Deep learning (deep machine learning, or deep structured learning, or hierarchical learning, or sometimes DL) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers with complex structures or otherwise, composed of multiple non-linear transformations. Deep learning is part of a broader family of machine learning methods based on learning representations of data.
  - [Wikipedia](https://www.wikiwand.com/en/Deep_learning)
 - [**Multilayer perceptron**](https://www.wikiwand.com/en/Multilayer_perceptron)
  - A multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one.
 - [**Recurrent Neural Network (RNN)**](https://www.wikiwand.com/en/Recurrent_neural_network)
  - A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior.
 - [**Restricted Boltzmann machine**](https://www.wikiwand.com/en/Restricted_Boltzmann_machine)
  - A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. RBMs were initially invented under the name Harmonium by Paul Smolensky in 1986, but only rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000s.
 - [**SOM**](https://www.wikiwand.com/en/Self-organizing_map)
  - A self-organizing map (SOM) or self-organising feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map. Self-organizing maps are different from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.
 - [**Convolutional neural network**](https://www.wikiwand.com/en/Convolutional_neural_network)
  - In machine learning, a convolutional neural network (CNN, or ConvNet) is a type of feed-forward artificial neural network where the individual neurons are tiled in such a way that they respond to overlapping regions in the visual field. Convolutional networks were inspired by biological processes and are variations of multilayer perceptrons which are designed to use minimal amounts of preprocessing.

### History  

Examinations of humans' central nervous systems inspired the concept of artificial neural networks. In an artificial neural network, simple artificial nodes, known as "neurons", "neurodes", "processing elements" or "units", are connected together to form a network which mimics a biological neural network.

### Models  
Neural network models in artificial intelligence are usually referred to as artificial neural networks (ANNs); these are essentially simple mathematical models defining a function  `f : X → Y`  or a distribution over `X` or both `X` and `Y`, but sometimes models are also intimately associated with a particular learning algorithm or learning rule. A common use of the phrase "ANN model" is really the definition of a class of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).

### Training Neural Nets
Most of the algorithms used in training artificial neural networks employ some form of gradient descent, using backpropagation to compute the actual gradients. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. The backpropagation training algorithms are usually classified into three categories: steepest descent (with variable learning rate, with variable learning rate and momentum, resilient backpropagation), quasi-Newton (Broyden-Fletcher-Goldfarb-Shanno, one step secant, Levenberg-Marquardt) and conjugate gradient (Fletcher-Reeves update, Polak-Ribiére update, Powell-Beale restart, scaled conjugate gradient)([src](https://www.wikiwand.com/en/Artificial_neural_network#Learning_algorithms)).

### Commonly Used Methods for Training Neural Nets
 - [**Evolutionary algorithm**](https://www.wikiwand.com/en/Evolutionary_methods) - In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection.
 - [**Gene expression programming**](https://www.wikiwand.com/en/Gene_expression_programming) - In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism.
 - [**Simulated annealing**](https://www.wikiwand.com/en/Simulated_annealing) - Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic for approximate global optimization in a large search space.
 - [**Expectation–maximization algorithm**](https://www.wikiwand.com/en/Expectation-maximization) - In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step.
 - [**Nonparametric statistics**](https://www.wikiwand.com/en/Non-parametric_methods) - Nonparametric statistics are statistics not based on parameterized families of probability distributions. They include both descriptive and inferential statistics.
 - [**Particle swarm optimization**](https://www.wikiwand.com/en/Particle_swarm_optimization) - In computer science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. PSO optimizes a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity.
 - 
 
### Real Life Applications
The tasks artificial neural networks are applied to tend to fall within the following broad categories:  
 - [Function approximation](#function_approximation), or [regression analysis](#regression), including [time series prediction](#time-series-prediction), [fitness approximation](#fitness-approximation) and modeling.
 - [Classification](#classification), including pattern and sequence recognition, [novelty detection](#novelty-detection) and sequential decision making.
 - Data processing, including filtering, clustering, [blind source separation](#blind) and compression.
 - Robotics, including directing manipulators, prosthesis.
 - Control, including [Computer numerical control](#cnc).
 - Vehicle control, trajectory prediction, process control, natural resources management, quantum chemistry, game-playing and decision making (backgammon, chess, poker), pattern recognition (radar systems, face identification, object recognition and more), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications (e.g. automated trading systems), data mining (or knowledge discovery in databases, "KDD"), visualization and e-mail spam filtering.
 - Artificial neural networks have also been used to diagnose several cancers. An ANN based hybrid lung cancer detection system named HLND improves the accuracy of diagnosis and the speed of lung cancer radiology. These networks have also been used to diagnose prostate cancer. The diagnoses can be used to make specific models taken from a large group of patients compared to information of one given patient. The models do not depend on assumptions about correlations of different variables. Colorectal cancer has also been predicted using the neural networks. Neural networks could predict the outcome for a patient with colorectal cancer with more accuracy than the current clinical methods. After training, the networks could predict multiple patient outcomes from unrelated institutions.

Types of Neural Nets
--------------------  
 - [**Recurrent neural network**](https://www.wikiwand.com/en/Recurrent_neural_network) - A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior.
  - **Examples:**
    - [**Long short-term memory (LSTM)**](https://www.wikiwand.com/en/Long_short_term_memory) - Long short-term memory (LSTM) is a recurrent neural network (RNN) architecture (an artificial neural network) published in 1997 by Sepp Hochreiter and Jürgen Schmidhuber. Like most RNNs, an LSTM network is universal in the sense that given enough network units it can compute anything a conventional computer can compute, provided it has the proper weight matrix, which may be viewed as its program.
    - [**Echo state network**](https://www.wikiwand.com/en/Echo_state_network) - The echo state network (ESN) is a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned.
    - [**Bi-directional RNN**](https://www.wikiwand.com/en/Types_of_artificial_neural_networks#/Bi-directional_RNN) - Invented by Schuster & Paliwal in 1997[22] bi-directional RNNs, or BRNNs, use a finite sequence to predict or label each element of the sequence based on both the past and the future context of the element. This is done by adding the outputs of two RNNs: one processing the sequence from left to right, the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM RNNs.
    - [**Hierarchical RNN**](https://www.wikiwand.com/en/Types_of_artificial_neural_networks#/Hierarchical_RNN) - There are many instances of hierarchical RNN whose elements are connected in various ways to decompose hierarchical behavior into useful subprograms.
    - [**Hopfield network**](https://www.wikiwand.com/en/Hopfield_network) - A Hopfield network is a form of recurrent artificial neural network popularized by John Hopfield in 1982, but described earlier by Little in 1974. Hopfield nets serve as content-addressable memory systems with binary threshold nodes. They are guaranteed to converge to a local minimum, but convergence to a false pattern (wrong local minimum) rather than the stored pattern (expected local minimum) can occur.
    - [**Boltzmann machine**](https://www.wikiwand.com/en/Boltzmann_machine) - A Boltzmann machine is a type of stochastic recurrent neural network invented by Geoffrey Hinton and Terry Sejnowski in 1985. Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets. They were one of the first examples of a neural network capable of learning internal representations, and are able to represent and (given sufficient time) solve difficult combinatoric problems.
    - [**Stochastic neural network**](https://www.wikiwand.com/en/Stochastic_neural_network) - Stochastic neural networks are a type of artificial neural networks built by introducing random variations into the network, either by giving the network's neurons stochastic transfer functions, or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help it escape from local minima.
 - [**Modular neural network**](https://www.wikiwand.com/en/Modular_neural_network) - A modular neural network is an artificial neural network characterized by a series of independent neural networks moderated by some intermediary. Each independent neural network serves as a module and operates on separate inputs to accomplish some subtask of the task the network hopes to perform.
  - **Examples**
    - [**Committee machine**](https://www.wikiwand.com/en/Committee_machine) - A committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response. The combined response of the committee machine is supposed to be superior to those of its constituent experts.
    - [**Associative neural network (ASNN)**](http://www.vcclab.org/lab/asnn/) - The ASNN is an extension of the committee of machines that goes beyond a simple/weighted average of different models. ASNN represents a combination of an ensemble of feedforward neural networks and the k-nearest neighbor technique (kNN). It uses the correlation between ensemble responses as a measure of distance amid the analyzed cases for the kNN. This corrects the bias of the neural network ensemble. An associative neural network has a memory that can coincide with the training set. If new data become available, the network instantly improves its predictive ability and provides data approximation (self-learn the data) without a need to retrain the ensemble. Another important feature of ASNN is the possibility to interpret neural network results by analysis of correlations between data cases in the space of models.
 - [**Neocognitron**](https://www.wikiwand.com/en/Neocognitron) - The neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in the 1980s. It has been used for handwritten character recognition and other pattern recognition tasks, and served as the inspiration for convolutional neural networks.
 - [**Feedforward neural network**](https://www.wikiwand.com/en/Feedforward_neural_network) - A feedforward neural network is an artificial neural network where connections between the units do not form a cycle. This is different from recurrent neural networks.
 - [**Multilayer perceptron**](https://www.wikiwand.com/en/Multilayer_perceptron) - A multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one.
 - [**Radial basis function (RBF) network**](https://www.wikiwand.com/en/Radial_basis_function_network) - In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters.
 - [**Self-organizing map**](https://www.wikiwand.com/en/Self-organizing_map) - A self-organizing map (SOM) or self-organising feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map. Self-organizing maps are different from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.
 - [**Physical neural network**](https://www.wikiwand.com/en/Physical_neural_network) - A physical neural network is a type of artificial neural network in which an electrically adjustable resistance material is used to emulate the function of a neural synapse. "Physical" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches which simulate neural networks.
 - **Other Types of Neural Networks**
  - **Examples**
    - [**Holographic associative memory**](https://www.wikiwand.com/en/Holographic_associative_memory) - Holographic Associative Memory is part of the family of analog, correlation-based, associative, stimulus-response memories, where information is mapped onto the phase orientation of complex numbers operating. It can be considered as a complex valued artificial neural network.
    - [**Instantaneously trained neural networks (ITNNs)**](https://www.wikiwand.com/en/Instantaneously_trained_neural_networks) - Instantaneously trained neural networks are feedforward artificial neural networks that create a new hidden neuron node for each novel training sample. The weights to this hidden neuron separate out not only this training sample but others that are near it, thus providing generalization.
    - [**Spiking neural network (SNNs)**](https://www.wikiwand.com/en/Spiking_neural_network) - Spiking neural networks (SNNs) fall into the third generation of neural network models, increasing the level of realism in a neural simulation. In addition to neuronal and synaptic state, SNNs also incorporate the concept of time into their operating model.
    - **Dynamic neural networks** - Dynamic neural networks not only deal with nonlinear multivariate behaviour, but also include (learning of) time-dependent behaviour such as various transient phenomena and delay effects. Techniques to estimate a system process from observed data fall under the general category of system identification.
    - [**Cascading neural networks**](http://web.cs.iastate.edu/~honavar/fahlman.pdf) - Cascade correlation is an architecture and supervised learning algorithm developed by Scott Fahlman, Scott E.; Lebiere, Christian (August 29, 1991). "The Cascade-Correlation Learning Architecture" (PDF). Carnegie Mellon University. Retrieved 4 October 2014. is an architecture and supervised learning algorithm developed by Scott Fahlman and Christian Lebiere. Instead of just adjusting the weights in a network of fixed topology, Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.
    - [**Neuro-fuzzy networks**](https://www.wikiwand.com/en/Neuro-fuzzy) - A neuro-fuzzy network is a fuzzy inference system in the body of an artificial neural network. Depending on the FIS type, there are several layers that simulate the processes involved in a fuzzy inference like fuzzification, inference, aggregation and defuzzification. Embedding an FIS in a general structure of an ANN has the benefit of using available ANN training methods to find the parameters of a fuzzy system.
    - [**Compositional pattern-producing network**](https://www.wikiwand.com/en/Neuro-fuzzy) - Compositional pattern-producing networks (CPPNs), are a variation of artificial neural networks (ANNs) which differ in their set of activation functions and how they are applied. While ANNs often contain only sigmoid functions and sometimes Gaussian functions, CPPNs can include both types of functions and many others.
    - [**One-shot associative memory**](https://courses.cit.cornell.edu/bionb330/readings/Associative%20Memories.pdf) - Where each learning pattern is presented once.
    - [**Hierarchical temporal memory**](https://www.wikiwand.com/en/Hierarchical_temporal_memory) - Hierarchical temporal memory (HTM) is an online machine learning model developed by Jeff Hawkins and Dileep George of Numenta, Inc. that models some of the structural and algorithmic properties of the neocortex. HTM is a biomimetic model based on the memory-prediction theory of brain function described by Jeff Hawkins in his book On Intelligence. HTM is a method for discovering and inferring the high-level causes of observed input patterns and sequences, thus building an increasingly complex model of the world.

      Jeff Hawkins states that HTM does not present any new idea or theory, but combines existing ideas to mimic the neocortex with a simple design that provides a large range of capabilities. HTM combines and extends approaches used in Bayesian networks, spatial and temporal clustering algorithms, while using a tree-shaped hierarchy of nodes that is common in neural networks.
 

 
Terms
-----  
 - [**Connectionism**](https://www.wikiwand.com/en/Connectionism) - Connectionism is a set of approaches in the fields of artificial intelligence, cognitive psychology, cognitive science, neuroscience, and philosophy of mind, that models mental or behavioral phenomena as the emergent processes of interconnected networks of simple units. The term was introduced by Donald Hebb in 1940s.
 - [**Support vector machine**](https://www.wikiwand.com/en/Support_vector_machine) - In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier.
 - [**Linear classifier**](https://www.wikiwand.com/en/Linear_classifier) - In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to. A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.
 - [**Deep learning**](https://www.wikiwand.com/en/Deep_learning) - Deep learning (deep machine learning, or deep structured learning, or hierarchical learning, or sometimes DL) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers with complex structures or otherwise, composed of multiple non-linear transformations. Deep learning is part of a broader family of machine learning methods based on learning representations of data.
 - [**Neuromorphic engineering**](https://www.wikiwand.com/en/Neuromorphic_computing) - Neuromorphic engineering, also known as neuromorphic computing, is a concept developed by Carver Mead, in the late 1980s, describing the use of very-large-scale integration (VLSI) systems containing electronic analog circuits to mimic neuro-biological architectures present in the nervous system. In recent times the term neuromorphic has been used to describe analog, digital, and mixed-mode analog/digital VLSI and software systems that implement models of neural systems (for perception, motor control, or multisensory integration).
 - [**Principal component analysis**](https://www.wikiwand.com/en/Principal_component) - Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables.
 - [**Convolution**](https://www.wikiwand.com/en/Convolution) - In mathematics and, in particular, functional analysis, convolution is a mathematical operation on two functions f and g, producing a third function that is typically viewed as a modified version of one of the original functions, giving the area overlap between the two functions as a function of the amount that one of the original functions is translated. Convolution is similar to cross-correlation.
 - [**Digital data**](https://www.wikiwand.com/en/Digital_data) - Digital data, in information theory and information systems, are discrete, discontinuous representations of information or works, as contrasted with continuous, or analog signals which behave in a continuous manner, or represent information using a continuous function. Although digital representations are the subject matter of discrete mathematics, the information represented can be either discrete, such as numbers and letters, or it can be continuous, such as sounds, images, and other measurements.
 - [**Analog signal**](https://www.wikiwand.com/en/Analog_signal) - An analog or analogue signal is any continuous signal for which the time varying feature (variable) of the signal is a representation of some other time varying quantity, i.e., analogous to another time varying signal. For example, in an analog audio signal, the instantaneous voltage of the signal varies continuously with the pressure of the sound waves.
 - [**Graphics processing unit (GPU)**](https://www.wikiwand.com/en/GPU) - A graphics processing unit (GPU), also occasionally called visual processing unit (VPU), is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles.
 - [**Visual cortex**](https://www.wikiwand.com/en/Visual_cortex) - The visual cortex of the brain is the part of the cerebral cortex responsible for processing visual information. This article addresses the ventral/dorsal model of the visual cortex.
 - [**Graphical model**](https://www.wikiwand.com/en/Graphical_models) - A graphical model or probabilistic graphical model (PGM) is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
 - [**Directed acyclic graph**](https://www.wikiwand.com/en/Directed_acyclic_graph) - In mathematics and computer science, a directed acyclic graph (DAG /ˈdæɡ/), is a directed graph with no directed cycles. That is, it is formed by a collection of vertices and directed edges, each edge connecting one vertex to another, such that there is no way to start at some vertex v and follow a sequence of edges that eventually loops back to v again.
 - [**Mathematical optimization**](https://www.wikiwand.com/en/Mathematical_optimization) - In mathematics, computer science and operations research, mathematical optimization (alternatively, optimization or mathematical programming) is the selection of a best element (with regard to some criteria) from some set of available alternatives. In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function.
 - [**Mean squared error**](https://www.wikiwand.com/en/Mean-squared_error) - In statistics, the mean squared error (MSE) of an estimator measures the average of the squares of the "errors", that is, the difference between the estimator and what is estimated. MSE is a risk function, corresponding to the expected value of the squared error loss or quadratic loss.
 <div id="regression"></div>
 - [**Regression analysis**](https://www.wikiwand.com/en/Regression_analysis) - Also known as _function approximation_, in Models, regression analysis is a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors').
 - [**Markov decision process (MDP)**](https://www.wikiwand.com/en/Markov_decision_process) - Markov decision processes (MDPs) provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning.
 - [**Markov chain (MC)**](https://www.wikiwand.com/en/Markov_chain) - A Markov chain (discrete-time Markov chain or DTMC), named after Andrey Markov, is a random process that undergoes transitions from one state to another on a state space. It must possess a property that is usually characterized as "memorylessness": the probability distribution of the next state depends only on the current state and not on the sequence of events that preceded it. *(The aim is to discover the policy (i.e., the MC) that minimizes the cost in Reinforcement Learning.)*
 - [**Bayesian probability**](https://www.wikiwand.com/en/Bayesian_probability) - Bayesian probability is one interpretation of the concept of probability. In contrast to interpreting probability as frequency or propensity of some phenomenon, Bayesian probability is a quantity that we assign to represent a state of knowledge, or a state of belief.
 - [**interpolation**](https://www.wikiwand.com/en/Interpolation) - a method of constructing new data points within the range of a discrete set of known data points.
 - [**Gaussian function**](http://www.wikiwand.com/en/Gaussian_function) - In mathematics, a Gaussian function, often simply referred to as a Gaussian, is a function of the form: ![alt text](http://i.imgur.com/qtRomXQ.png) for arbitrary real constants a, b and c.
 - [**Sigmoid function (aka logistic function)**](https://www.wikiwand.com/en/Sigmoid_function) - A sigmoid function is a mathematical function having an "S" shape (sigmoid curve).
 - [**Tikhonov regularization (aka ridge regression)**](https://www.wikiwand.com/en/Ridge_regression) - Shrinkage technique. Tikhonov regularization, named for Andrey Tikhonov, is the most commonly used method of regularization of ill-posed problems. In statistics, the method is known as ridge regression, and with multiple independent discoveries, it is also variously known as the Tikhonov–Miller method, the Phillips–Twomey method, the constrained linear inversion method, and the method of linear regularization.
 - [**Iteratively reweighted least squares**](https://www.wikiwand.com/en/Iteratively_reweighted_least_squares) - The method of iteratively reweighted least squares (IRLS) is used to solve certain optimization problems with objective functions of the form: ![alt text](http://i.imgur.com/WQW3XzD.png) by an iterative method in which each step involves solving a weighted least squares problem of the form: ![alt text](http://i.imgur.com/pcu4Tqn.png) IRLS is used to find the maximum likelihood estimates of a generalized linear model, and in robust regression to find an M-estimator, as a way of mitigating the influence of outliers in an otherwise normally-distributed data set. 
 - [**Overfitting**](https://www.wikiwand.com/en/Overfitting) - In statistics and machine learning, overfitting occurs when a statistical model describes random error or noise instead of the underlying relationship. Overfitting generally occurs when a model is excessively complex, such as having too many parameters relative to the number of observations.
 - [**Pulse computation**](https://www.wikiwand.com/en/Pulse_computer) - Pulse computation is a hybrid of digital and analog computation that uses aperiodic electrical spikes, as opposed to the periodic voltages in a digital computer or the continuously varying voltages in on analog computer. Pulse streams are unlocked, so they can arrive at arbitrary times and can be generated by analog processes, although each spike is allocated a binary value, as it would be in a digital computer.
 - [**Polychronize**](http://senselab.med.yale.edu/ModelDB/showmodel.cshtml?model=115968) - exhibit reproducible time-locked but not synchronous firing patterns with millisecond precision.
 <div id="function_approximation"></div>
 - [**Function approximation**](https://www.wikiwand.com/en/Function_approximation) - The need for function approximations arises in many branches of applied mathematics, and computer science in particular. In general, a function approximation problem asks us to select a function among a well-defined class that closely matches ("approximates") a target function in a task-specific way.
 <div id="time-series-prediction"></div>
 - [**Time series prediction**](https://www.wikiwand.com/en/Time_series_prediction) - A time series is a sequence of data points, typically consisting of successive measurements made over a time interval. Examples of time series are ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.
 <div id="fitness-approximation"></div>
 - [**Fitness approximation**](https://www.wikiwand.com/en/Fitness_approximation) - In function optimization, fitness approximation is a method for decreasing the number of fitness function evaluations to reach a target solution.
 <div id="classification"></div>
 - [**Statistical classification**](https://www.wikiwand.com/en/Statistical_classification) - In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into "spam" or "non-spam" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.).
 <div id="novelty-detection"></div>
 - [**Novelty detection**](https://www.wikiwand.com/en/Novelty_detection) - Novelty detection is the identification of new or unknown data that a machine learning system has not been trained with and was not previously aware of, with the help of either statistical or machine learning based approaches. Novelty detection is one of the fundamental requirements of a good classification system.
 <div id="blind"></div>
 - [**Blind signal separation**](https://www.wikiwand.com/en/Blind_source_separation) - Blind signal separation, also known as blind source separation, is the separation of a set of source signals from a set of mixed signals, without the aid of information (or with very little information) about the source signals or the mixing process. This problem is in general highly underdetermined, but useful solutions can be derived under a surprising variety of conditions.
 <div id="cnc"></div>
 - [**Numerical control**](https://www.wikiwand.com/en/Computer_numerical_control) - Numerical control (NC) is the automation of machine tools that are operated by precisely programmed commands encoded on a storage medium, as opposed to controlled manually by hand wheels or levers, or mechanically automated by cams alone. Most NC today is computer (or computerized) numerical control (CNC), in which computers play an integral part of the control.
 - [**Regularization (mathematics)**](https://www.wikiwand.com/en/Regularization_(mathematics)) - Regularization, in mathematics and statistics and particularly in the fields of machine learning and inverse problems, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. This information is usually of the form of a penalty for complexity, such as restrictions for smoothness or bounds on the vector space norm.

Algorithms
----------  
 - [**Perceptron**](https://www.wikiwand.com/en/Perceptron) - In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another. It is a type of linear classifier
 - [**Gradient descent**](https://www.wikiwand.com/en/Gradient_descent) - Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point.
 - [**Backpropagation**](https://www.wikiwand.com/en/Backpropagation) - Backpropagation, an abbreviation for "backward propagation of errors", is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network.
 - [**Expectation–maximization algorithm**](https://www.wikiwand.com/en/Expectation-maximization) - In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step.
 - [**Evolutionary algorithm**](https://www.wikiwand.com/en/Evolutionary_methods) - In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection.
 - [**Learning vector quantization**](https://www.wikiwand.com/en/Learning_Vector_Quantization) - In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.
 - [**K-nearest neighbors algorithm**](https://www.wikiwand.com/en/Nearest_neighbor_(pattern_recognition)) - In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space.
 

Examples
--------  

Resources
---------  
 - [**MNIST database (Mixed National Institute of Standards and Technology database)**](https://www.wikiwand.com/en/MNIST_database) - The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.
 - **Article:** [**Comparison of Feed-Forward Neural Network Training Algorithms for Oscillometric Blood Pressure Estimation**](http://www.researchgate.net/publication/224173336_Comparison_of_Feed-Forward_Neural_Network_training_algorithms_for_oscillometric_blood_pressure_estimation)

Important People 
----------------  
 - [Warren McCulloch](https://www.wikiwand.com/en/Warren_McCulloch) and [Walter Pitts](https://www.wikiwand.com/en/Walter_Pitts) - created a computational model for neural networks based on mathematics and algorithms called threshold logic. ([see here](https://www.wikiwand.com/en/Artificial_neural_network#/History)) 
 - [Donald Hebb](https://www.wikiwand.com/en/Donald_Hebb)  
 - [Wesley A. Clark](https://www.wikiwand.com/en/Wesley_A._Clark)  
 - [Frank Rosenblatt](https://www.wikiwand.com/en/Frank_Rosenblatt)  
 - [Paul Werbos](https://www.wikiwand.com/en/Paul_Werbos) - created the _backpropagation algorithm_ in 1975. 
 - [Jürgen Schmidhuber](https://www.wikiwand.com/en/J%C3%BCrgen_Schmidhuber)  
 - Dan Ciresan
 - [Yann LeCun](https://www.wikiwand.com/en/Yann_LeCun)
 - Kunihiko Fukushima
 - [Torsten Wiesel](https://www.wikiwand.com/en/Torsten_Wiesel)
 - [David H. Hubel](https://www.wikiwand.com/en/David_H._Hubel)
 - [Dimitri Bertsekas](https://www.wikiwand.com/en/Dimitri_Bertsekas)


Conferences  
-----------  
 - [International Conference on Document Analysis and Recognition (ICDAR)](icdar.org)
 - 


Groups
------  


Publications/Journals/Media
---------------------------  
 - [**ACM Digital Library: Neural Nets**](http://dl.acm.org/results.cfm?query=neural+nets&Go.x=0&Go.y=0)

Companies
---------  


Universities/Research Institutions
----------------------------------
 - [**Dalle Molle Institute for Artificial Intelligence Research (IDSIA)**](https://www.wikiwand.com/en/IDSIA) - The Dalle Molle Institute for Artificial Intelligence Research (Italian: Istituto Dalle Molle di Studi sull'Intelligenza Artificiale; IDSIA) was founded in 1988 by Angelo Dalle Molle through the private Fondation Dalle Molle. In 2000 it became a public research institute, affiliated with the University of Lugano and SUPSI in Ticino, Switzerland.
 - **University of Toronto**
 - 



Artificial Neural Networks (ANN)
================================
### (aka Neural Nets)

In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) and are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other. The connections have numeric weights that can be tuned based on experience, making neural nets adaptive to inputs and capable of learning.  Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and the biological architecture of the brain is debated; it's not clear to what degree artificial neural networks mirror brain function ([src](http://uhaweb.hartford.edu/compsci/neural-networks-definition.html)).

An ANN is typically defined by three types of parameters:
 1. The interconnection pattern between the different layers of neurons
 2. The learning process for updating the weights of the interconnections
 3. The activation function that converts a neuron's weighted input to its output activation.

### History  

Examinations of humans' central nervous systems inspired the concept of artificial neural networks. In an artificial neural network, simple artificial nodes, known as "neurons", "neurodes", "processing elements" or "units", are connected together to form a network which mimics a biological neural network.

### Models  
Neural network models in artificial intelligence are usually referred to as artificial neural networks (ANNs); these are essentially simple mathematical models defining a function  `f : X → Y`  or a distribution over `X` or both `X` and `Y`, but sometimes models are also intimately associated with a particular learning algorithm or learning rule. A common use of the phrase "ANN model" is really the definition of a class of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).



Types of Neural Nets
--------------------  
 - [**Recurrent neural network**](https://www.wikiwand.com/en/Recurrent_neural_network) - A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior.
  - Examples:
   - [Long short-term memory (LSTM)](https://www.wikiwand.com/en/Long_short_term_memory) - Long short-term memory (LSTM) is a recurrent neural network (RNN) architecture (an artificial neural network) published in 1997 by Sepp Hochreiter and Jürgen Schmidhuber. Like most RNNs, an LSTM network is universal in the sense that given enough network units it can compute anything a conventional computer can compute, provided it has the proper weight matrix, which may be viewed as its program.
 - [Neocognitron](https://www.wikiwand.com/en/Neocognitron) - The neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in the 1980s. It has been used for handwritten character recognition and other pattern recognition tasks, and served as the inspiration for convolutional neural networks.
 - [**
 
 
 
Terms
-----  
 - [**Connectionism**](https://www.wikiwand.com/en/Connectionism) - Connectionism is a set of approaches in the fields of artificial intelligence, cognitive psychology, cognitive science, neuroscience, and philosophy of mind, that models mental or behavioral phenomena as the emergent processes of interconnected networks of simple units. The term was introduced by Donald Hebb in 1940s.
 - [**Support vector machine**](https://www.wikiwand.com/en/Support_vector_machine) - In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier.
 - [**Linear classifier**](https://www.wikiwand.com/en/Linear_classifier) - In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to. A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.
 - [**Deep learning**](https://www.wikiwand.com/en/Deep_learning) - Deep learning (deep machine learning, or deep structured learning, or hierarchical learning, or sometimes DL) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers with complex structures or otherwise, composed of multiple non-linear transformations. Deep learning is part of a broader family of machine learning methods based on learning representations of data.
 - [**Neuromorphic engineering**](https://www.wikiwand.com/en/Neuromorphic_computing) - Neuromorphic engineering, also known as neuromorphic computing, is a concept developed by Carver Mead, in the late 1980s, describing the use of very-large-scale integration (VLSI) systems containing electronic analog circuits to mimic neuro-biological architectures present in the nervous system. In recent times the term neuromorphic has been used to describe analog, digital, and mixed-mode analog/digital VLSI and software systems that implement models of neural systems (for perception, motor control, or multisensory integration).
 - [**Principal component analysis**](https://www.wikiwand.com/en/Principal_component) - Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables.
 - [**Convolution**](https://www.wikiwand.com/en/Convolution) - In mathematics and, in particular, functional analysis, convolution is a mathematical operation on two functions f and g, producing a third function that is typically viewed as a modified version of one of the original functions, giving the area overlap between the two functions as a function of the amount that one of the original functions is translated. Convolution is similar to cross-correlation.
 - [**Digital data**](https://www.wikiwand.com/en/Digital_data) - Digital data, in information theory and information systems, are discrete, discontinuous representations of information or works, as contrasted with continuous, or analog signals which behave in a continuous manner, or represent information using a continuous function. Although digital representations are the subject matter of discrete mathematics, the information represented can be either discrete, such as numbers and letters, or it can be continuous, such as sounds, images, and other measurements.
 - [**Analog signal**](https://www.wikiwand.com/en/Analog_signal) - An analog or analogue signal is any continuous signal for which the time varying feature (variable) of the signal is a representation of some other time varying quantity, i.e., analogous to another time varying signal. For example, in an analog audio signal, the instantaneous voltage of the signal varies continuously with the pressure of the sound waves.
 - [**Graphics processing unit (GPU)**](https://www.wikiwand.com/en/GPU) - A graphics processing unit (GPU), also occasionally called visual processing unit (VPU), is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles.
 - [**Visual cortex**](https://www.wikiwand.com/en/Visual_cortex) - The visual cortex of the brain is the part of the cerebral cortex responsible for processing visual information. This article addresses the ventral/dorsal model of the visual cortex.
 - [**Graphical model**](https://www.wikiwand.com/en/Graphical_models) - A graphical model or probabilistic graphical model (PGM) is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
 - [**

Algorithms
----------  
 - [**Perceptron**](https://www.wikiwand.com/en/Perceptron) - In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another. It is a type of linear classifier
 - [**
 
 
Examples
--------  

Resources
---------  
 - [**MNIST database (Mixed National Institute of Standards and Technology database)**](https://www.wikiwand.com/en/MNIST_database) - The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.
 - 

Important People 
----------------  
 - [Warren McCulloch](https://www.wikiwand.com/en/Warren_McCulloch) and [Walter Pitts](https://www.wikiwand.com/en/Walter_Pitts) - created a computational model for neural networks based on mathematics and algorithms called threshold logic. ([see here](https://www.wikiwand.com/en/Artificial_neural_network#/History)) 
 - [Donald Hebb](https://www.wikiwand.com/en/Donald_Hebb)  
 - [Wesley A. Clark](https://www.wikiwand.com/en/Wesley_A._Clark)  
 - [Frank Rosenblatt](https://www.wikiwand.com/en/Frank_Rosenblatt)  
 - [Paul Werbos](https://www.wikiwand.com/en/Paul_Werbos) - created the _backpropagation algorithm_ in 1975. 
 - [Jürgen Schmidhuber](https://www.wikiwand.com/en/J%C3%BCrgen_Schmidhuber)  
 - Dan Ciresan
 - [Yann LeCun](https://www.wikiwand.com/en/Yann_LeCun)
 - Kunihiko Fukushima
 - [Torsten Wiesel](https://www.wikiwand.com/en/Torsten_Wiesel)
 - [David H. Hubel](https://www.wikiwand.com/en/David_H._Hubel)


Conferences  
-----------  
 - [International Conference on Document Analysis and Recognition (ICDAR)](icdar.org)
 - 


Groups
------  


Publications/Journals/Media
---------------------------  

Companies
---------  


Universities/Research Institutions
----------------------------------
 - [**Dalle Molle Institute for Artificial Intelligence Research (IDSIA)**](https://www.wikiwand.com/en/IDSIA) - The Dalle Molle Institute for Artificial Intelligence Research (Italian: Istituto Dalle Molle di Studi sull'Intelligenza Artificiale; IDSIA) was founded in 1988 by Angelo Dalle Molle through the private Fondation Dalle Molle. In 2000 it became a public research institute, affiliated with the University of Lugano and SUPSI in Ticino, Switzerland.
 - **University of Toronto**
 - 


